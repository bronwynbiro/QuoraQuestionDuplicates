{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Duplicates - Feature Engineering\n",
    "\n",
    "Our goal is to identify which questions asked on [Quora](https://www.quora.com/), a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, jaccard, euclidean\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "DELTA = 0.0001\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "        \n",
    "    # Remove stop words\n",
    "    words = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stops]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # Clean the text of special characters\n",
    "    text = text.replace(\"%\", \" percent \").replace(\"₹₹\", \" rupee \").replace(\"$\", \" dollar \").replace(\"€\", \" euro \")\n",
    "    \n",
    "    # Expand abbreviations\n",
    "    text = re.sub(r\"\\b([A-Za-z]+)'re\\b\", '\\\\1 are', text)\n",
    "    text = re.sub(r\"\\b([A-Za-z]+)'s\\b\", '\\\\1 is', text)\n",
    "    text = re.sub(r\"\\b([A-Za-z]+)'ve\\b\", '\\\\1 have', text)\n",
    "    text = re.sub(r\"([0-9]+)000000\", r\"\\1m\", text)\n",
    "    text = re.sub(r\"([0-9]+)000\", r\"\\1k\", text)\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Shorten words to their stems\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_new_features(q1, q2):\n",
    "    new_features = [0.0]*15\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return new_features\n",
    "\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    q1_stop_words = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stop_words = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stop_words.intersection(q2_stop_words))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    q1_avg_word_len = sum(len(word) for word in q1_words) / (len(q1_words) + DELTA)\n",
    "    q2_avg_word_len = sum(len(word) for word in q2_words) / (len(q2_words) + DELTA)\n",
    "    \n",
    "    q1_avg_stop_len = sum(len(word) for word in q1_stop_words) / (len(q1_stop_words) + DELTA)\n",
    "    q2_avg_stop_len = sum(len(word) for word in q2_stop_words) / (len(q2_stop_words) + DELTA)\n",
    "    \n",
    "    # Common words, stop words, and token ratios\n",
    "    new_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + DELTA)\n",
    "    new_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + DELTA)\n",
    "    new_features[2] = common_stop_count / (min(len(q1_stop_words), len(q2_stop_words)) + DELTA)\n",
    "    new_features[3] = common_stop_count / (max(len(q1_stop_words), len(q2_stop_words)) + DELTA)\n",
    "    new_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + DELTA)\n",
    "    new_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + DELTA)\n",
    "    \n",
    "    # Matching first or last token \n",
    "    new_features[6] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    new_features[7] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # Difference in token lengths, average word lengths\n",
    "    new_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    new_features[9] = abs(q1_avg_word_len - q2_avg_word_len)\n",
    "    new_features[10] = abs(q1_avg_stop_len - q1_avg_stop_len)\n",
    "    \n",
    "    # Average of token length\n",
    "    new_features[11] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    # Similarity measures\n",
    "    new_features[12] = SequenceMatcher(None, \" \".join(q1_words), \" \".join(q2_words)).ratio()\n",
    "    new_features[13] = SequenceMatcher(None, \" \".join(q1_stop_words), \" \".join(q2_stop_words)).ratio()\n",
    "    \n",
    "    return new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_features(df):\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess_text)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess_text)\n",
    "    \n",
    "    new_features = df.apply(lambda x: get_new_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"min_common_word_count\"] = list(map(lambda x: x[0], new_features))\n",
    "    df[\"max_common_word_count\"] = list(map(lambda x: x[1], new_features))\n",
    "    df[\"min_common_stop_count\"] = list(map(lambda x: x[2], new_features))\n",
    "    df[\"max_common_stop_count\"]  = list(map(lambda x: x[3], new_features))\n",
    "    df[\"min_common_token_count\"] = list(map(lambda x: x[4], new_features))\n",
    "    df[\"max_common_token_count\"] = list(map(lambda x: x[5], new_features))\n",
    "    df[\"same_first_word\"] = list(map(lambda x: x[6], new_features))\n",
    "    df[\"same_last_word\"] = list(map(lambda x: x[7], new_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], new_features))\n",
    "    df[\"avg_word_len_diff\"]  = list(map(lambda x: x[9], new_features))\n",
    "    df[\"avg_stop_len_diff\"]  = list(map(lambda x: x[10], new_features))\n",
    "    df[\"mean_len\"]  = list(map(lambda x: x[11], new_features))\n",
    "    df[\"word_similarity\"]  = list(map(lambda x: x[12], new_features))\n",
    "    df[\"stop_similarity\"]  = list(map(lambda x: x[13], new_features))\n",
    "    df[\"tf_idf\"]  = list(map(lambda x: x[14], new_features))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running NLP features on train..\n",
      "Running NLP features on test..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running NLP features on train..\")\n",
    "train_df = pd.read_csv(\"input/train.csv\")\n",
    "train_df = nlp_features(train_df)\n",
    "train_df.drop([\"id\", \"qid1\", \"qid2\", \"question1\", \"question2\", \"is_duplicate\"], axis=1, inplace=True)\n",
    "train_df.to_csv(\"input/nlp_features_train.csv\", index=False)\n",
    "\n",
    "print(\"Running NLP features on test..\")\n",
    "test_df = pd.read_csv(\"input/test.csv\")\n",
    "test_df = nlp_features(test_df)\n",
    "test_df.drop([\"test_id\", \"question1\", \"question2\"], axis=1, inplace=True)\n",
    "test_df.to_csv(\"input/nlp_features_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
